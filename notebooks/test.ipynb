{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/predicates_clean.json\") as f:\n",
    "    preds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "similarity = fuzz.partial_ratio(\"MPA film rating\", \"What is the MPA film rating of Avatar?\")\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relation(question) -> list:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        doc = nlp(question)\n",
    "        relations = None\n",
    "        for token in doc:\n",
    "            print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "                    token.shape_, token.is_alpha, token.is_stop)\n",
    "        try:\n",
    "            similarities = {}\n",
    "            for v in preds.values():\n",
    "                similarity = fuzz.partial_ratio(v, question)\n",
    "                if similarity > 65:\n",
    "                    similarities[v] = similarity\n",
    "\n",
    "            similarities = {k: v for k, v in sorted(similarities.items(), key=lambda item: item[1])}  \n",
    "            relations = list(similarities.keys())[-1]         \n",
    "            assert relations, \"No exact match found\"\n",
    "        \n",
    "        except :\n",
    "            try:\n",
    "                relations = [\n",
    "                    tok.lemma_\n",
    "                    for tok in doc\n",
    "                    if tok.dep_ in (\"attr\", \"nsubj\") and tok.pos_ in (\"PROPN\", \"NOUN\")\n",
    "                ]\n",
    "                assert relations, \"No relation found ...\"\n",
    "                relations = \" \".join(relations)  \n",
    "\n",
    "            except AssertionError:\n",
    "                print(2)\n",
    "                relations = [\n",
    "                    tok.lemma_\n",
    "                    for tok in doc\n",
    "                    if tok.pos_ in (\"VERB\")\n",
    "                ]\n",
    "        \n",
    "        finally:\n",
    "            return relations    \n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who who PRON WP nsubj Xxx True True\n",
      "directed direct VERB VBD ROOT xxxx True False\n",
      "Titanic Titanic PROPN NNP dobj Xxxxx True False\n",
      "? ? PUNCT . punct ? False False\n",
      "director\n"
     ]
    }
   ],
   "source": [
    "print(get_relation(\"Who directed Titanic?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import sys\n",
    "sys.path.append(\"../models/\")\n",
    "import NER_CRF\n",
    "\n",
    "\n",
    "class NLP_Operations:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def get_ner(self, question):\n",
    "        ner = NER_CRF.get_ner(question)\n",
    "        return ner\n",
    "\n",
    "    def get_relation(self, question):\n",
    "        doc = self.nlp(question)\n",
    "        relations = [\n",
    "            tok.lemma_\n",
    "            for tok in doc\n",
    "            if tok.dep_ in (\"attr\", \"nsubj\") and tok.pos_ in (\"NOUN\")\n",
    "        ]\n",
    "        return relations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def get_dicti(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        dicti = json.load(f)\n",
    "    return dicti\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "\n",
    "\n",
    "class GraphOperations:\n",
    "    def __init__(self, graph_file=\"data/14_graph.nt\"):\n",
    "        self.graph = rdflib.Graph()\n",
    "        # self.load_graph(graph_file)\n",
    "\n",
    "    def load_graph(self, graph_file):\n",
    "        print(\"loading graph ...\")\n",
    "        self.graph.parse(graph_file, format=\"turtle\")\n",
    "        print(\"loaded graph successfully!\")\n",
    "\n",
    "    def query(self, message):\n",
    "        # remember to delete these 2 lines after this boring evaluation\n",
    "        message = message.replace('\"\"\"', \"\").replace(\"'''\", \"\")\n",
    "        print(\"message in sparql\")\n",
    "        message = str(message)\n",
    "        print(message)\n",
    "        temp = [str(s) for s, in self.graph.query(message)]\n",
    "        print(temp)\n",
    "        return temp\n",
    "\n",
    "    def query2(self, message):\n",
    "        return calculate_answer(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.append(\"../usecases/\")\n",
    "# import utils\n",
    "# import nlp_operations\n",
    "# import graph_operations\n",
    "import copy\n",
    "import editdistance\n",
    "\n",
    "\n",
    "class AnswerCalculator:\n",
    "    def __init__(self):\n",
    "        self.nodes = get_dicti(\"../data/nodes.json\")\n",
    "        self.predicates = get_dicti(\"../data/predicates_clean.json\")\n",
    "        self.nlp_operator = NLP_Operations()\n",
    "        self.graph_operator = GraphOperations()\n",
    "        self.wh_words = [\n",
    "            \"What\",\n",
    "            \"what\",\n",
    "            \"when\",\n",
    "            \"When\",\n",
    "            \"where\",\n",
    "            \"Where\",\n",
    "            \"Who\",\n",
    "            \"who\",\n",
    "            \"Whom\",\n",
    "            \"whom\",\n",
    "            \"Which\",\n",
    "            \"which\",\n",
    "            \"Whose\",\n",
    "            \"whose\",\n",
    "            \"Why\",\n",
    "            \"why\",\n",
    "            \"How\",\n",
    "            \"how\",\n",
    "        ]\n",
    "        self.useless_words = [\n",
    "            \"am\",\n",
    "            \"is\",\n",
    "            \"are\",\n",
    "            \"was\",\n",
    "            \"were\",\n",
    "            \"a\",\n",
    "            \"an\",\n",
    "            \"the\",\n",
    "            \"that\",\n",
    "            \"this\",\n",
    "            \"these\",\n",
    "            \"those\",\n",
    "            \"above\",\n",
    "            \"across\",\n",
    "            \"against\",\n",
    "            \"along\",\n",
    "            \"among\",\n",
    "            \"around\",\n",
    "            \"at\",\n",
    "            \"before\",\n",
    "            \"behind\",\n",
    "            \"below\",\n",
    "            \"beneath\",\n",
    "            \"beside\",\n",
    "            \"between\",\n",
    "            \"by\",\n",
    "            \"down\",\n",
    "            \"from\",\n",
    "            \"in\",\n",
    "            \"into\",\n",
    "            \"near\",\n",
    "            \"off\",\n",
    "            \"on\",\n",
    "            \"to\",\n",
    "            \"woward\",\n",
    "            \"under\",\n",
    "            \"upon\",\n",
    "            \"with\",\n",
    "            \"and\",\n",
    "            \"within\",\n",
    "            \"of\",\n",
    "            \"for\",\n",
    "            \"since\",\n",
    "            \"during\",\n",
    "            \"over\",\n",
    "        ]\n",
    "        self.all_delete_words = self.wh_words + self.useless_words\n",
    "\n",
    "    def calculate_answer(self, question):\n",
    "        question_list = question.split(\" \")\n",
    "        tag_list = self.nlp_operator.get_ner(\n",
    "            question\n",
    "        )  # returns e.g. [['O', 'O', 'O', 'O', 'O']]\n",
    "\n",
    "        print(\"tag_list: \", tag_list)\n",
    "        wh_word = question_list[0].upper()\n",
    "        if wh_word == \"WHEN\":\n",
    "            return self.calculate_when_answer(\n",
    "                copy.deepcopy(question), tag_list[0]\n",
    "            )  # this is supposed to be tag_list, right?\n",
    "        else:\n",
    "            return self.calculate_other_answer(copy.deepcopy(question), tag_list[0])\n",
    "\n",
    "    # this is also not a question_list but a question\n",
    "    def calculate_other_answer(self, question, tag_list):\n",
    "        print(\"calculate other answer\")\n",
    "        question_list = question.split(\" \")\n",
    "        print(tag_list)\n",
    "        try:\n",
    "            # find entity\n",
    "            indexes = [index for index, val in enumerate(tag_list) if val != \"O\"]\n",
    "            print(indexes)\n",
    "            entity = (\n",
    "                \" \".join(question_list[indexes[0] : indexes[-1] + 1])\n",
    "                .rstrip(\"?\")\n",
    "            )\n",
    "            print(entity)\n",
    "\n",
    "            relations = self.nlp_operator.get_relation(question)\n",
    "            print(relations)\n",
    "            assert len(relations) == 1\n",
    "            relations = relations[0]\n",
    "\n",
    "            possible_answer = self.search_answer(entity, relations, 0)\n",
    "\n",
    "        except:\n",
    "            temp = copy.deepcopy(question_list)\n",
    "            possible_answer = self.forcely_search(temp, 0)\n",
    "\n",
    "        finally:\n",
    "            return possible_answer\n",
    "\n",
    "    # you are not passing the question list, but question?\n",
    "    def calculate_when_answer(self, question_list, tag_list):\n",
    "        try:\n",
    "            # find entity\n",
    "            indexes = [index for index, val in enumerate(tag_list) if val != \"O\"]\n",
    "            entity = (\n",
    "                \" \".join(question_list[indexes[0] : indexes[-1] + 1])\n",
    "                .rstrip(\"?\")\n",
    "                .rstrip('\"')\n",
    "                .rstrip(\"'\")\n",
    "            )\n",
    "\n",
    "            entity_list = \" \".join(question_list[indexes[0] : indexes[-1] + 1]).split(\n",
    "                \" \"\n",
    "            )\n",
    "\n",
    "            # delete entity word\n",
    "            temp = copy.deepcopy(question_list)\n",
    "            for word in entity_list:\n",
    "                temp.remove(word)\n",
    "\n",
    "            for word in self.all_delete_words:\n",
    "                try:\n",
    "                    temp.remove(word)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            assert len(temp) == 1\n",
    "            relations = temp[0] + \" time\"\n",
    "\n",
    "            possible_answer = []\n",
    "            possible_answer = self.search_answer(entity, relations, 1)\n",
    "\n",
    "        except:\n",
    "            temp = copy.deepcopy(question_list)\n",
    "            possible_answer = self.forcely_search(temp, 1)\n",
    "\n",
    "        return possible_answer\n",
    "\n",
    "    def search_answer(self, entity_word, related_word, is_when):\n",
    "        search_loop = 0\n",
    "        search_flag = 0\n",
    "        edit_distance = 1\n",
    "\n",
    "        node_distance_dict = self.calculate_node_distance(entity_word)\n",
    "        pred_distance_dict = self.calculate_pred_distance(related_word)\n",
    "\n",
    "        searched_answers = []\n",
    "        for n_key in node_distance_dict.keys():\n",
    "            search_loop += 1\n",
    "\n",
    "            if node_distance_dict[n_key] == 0:\n",
    "                edit_distance = 0\n",
    "            else:\n",
    "                edit_distance = 1\n",
    "\n",
    "            for p_key in pred_distance_dict.keys():                \n",
    "                if is_when:\n",
    "                    query = f'''\n",
    "                        PREFIX ddis: <http://ddis.ch/atai/>\n",
    "                        PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "                        PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "                        PREFIX schema: <http://schema.org/>\n",
    "                        SELECT ?date WHERE{{    \n",
    "                            wd:{n_key} wdt:{p_key} ?date.\n",
    "                        }}\n",
    "                        LIMIT 1\n",
    "                        '''\n",
    "                else:\n",
    "                    query = f'''\n",
    "                        PREFIX ddis: <http://ddis.ch/atai/>\n",
    "                        PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "                        PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "                        PREFIX schema: <http://schema.org/>\n",
    "                        SELECT ?entity_name WHERE{{    \n",
    "                            wd:{n_key} wdt:{p_key} ?temp.\n",
    "                            ?temp rdfs:label ?entity_name.\n",
    "                        }}\n",
    "                        LIMIT 1\n",
    "                    '''\n",
    "\n",
    "                print(query)\n",
    "\n",
    "                answers = graph_operator.query(query)\n",
    "                print(answers)\n",
    "                if len(answers) > 0:\n",
    "                    search_flag = 1\n",
    "                if search_flag == 1:\n",
    "                    searched_answers.append(answers)\n",
    "                    break\n",
    "            if search_flag == 1 and edit_distance == 1:\n",
    "                break\n",
    "            if search_loop > 10:\n",
    "                answers = []\n",
    "                break\n",
    "\n",
    "        return searched_answers\n",
    "\n",
    "    def search_answer_for_all_O(self, entity_word, related_word, is_when):\n",
    "        node_distance_dict = self.calculate_node_distance(entity_word)\n",
    "        pred_distance_dict = self.calculate_pred_distance(related_word)\n",
    "\n",
    "        search_flag = 0\n",
    "\n",
    "        # entity distance seems reliable, try 5 times\n",
    "        try_times = 0\n",
    "        for n_key in node_distance_dict.keys():\n",
    "            try_times += 1\n",
    "            for p_key in pred_distance_dict.keys():\n",
    "                if is_when:\n",
    "                    query = f'''\n",
    "                        PREFIX ddis: <http://ddis.ch/atai/>\n",
    "                        PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "                        PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "                        PREFIX schema: <http://schema.org/>\n",
    "                        SELECT ?date WHERE{{    \n",
    "                            wd:{n_key} wdt:{p_key} ?date.\n",
    "                        }}\n",
    "                        LIMIT 1\n",
    "                        '''\n",
    "                else:\n",
    "                    query = f'''\n",
    "                        PREFIX ddis: <http://ddis.ch/atai/>\n",
    "                        PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "                        PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "                        PREFIX schema: <http://schema.org/>\n",
    "                        SELECT ?entity_name WHERE{{    \n",
    "                            wd:{n_key} wdt:{p_key} ?temp.\n",
    "                            ?temp rdfs:label ?entity_name.\n",
    "                        }}\n",
    "                        LIMIT 1\n",
    "                    '''\n",
    "\n",
    "                answers = [str(s) for s, in g.query(query)]\n",
    "                if len(answers) > 0 and len(answers[0]) == 10:\n",
    "                    search_flag = 1\n",
    "                if search_flag == 1:\n",
    "                    break\n",
    "            if search_flag == 1:\n",
    "                break\n",
    "            if try_times > 10:\n",
    "                break\n",
    "        return answers\n",
    "\n",
    "    def calculate_node_distance(self, word):\n",
    "        distance_dict = {}\n",
    "        print(\"entity matching for {}\".format(word))\n",
    "        for key, value in self.nodes.items():\n",
    "            distance_dict[key.split(\"/\")[-1]] = editdistance.eval(value, word)\n",
    "        distance_dict = dict(sorted(distance_dict.items(), key=lambda x: x[1]))\n",
    "        return distance_dict\n",
    "\n",
    "    def calculate_pred_distance(self, related_word):\n",
    "        pred_distance_dict = {}\n",
    "        print(\"relation matching for {}\".format(related_word))\n",
    "        for key, value in self.predicates.items():\n",
    "            pred_distance_dict[key.split(\"/\")[-1]] = editdistance.eval(\n",
    "                value, related_word\n",
    "            )\n",
    "        pred_distance_dict = dict(\n",
    "            sorted(pred_distance_dict.items(), key=lambda x: x[1])\n",
    "        )\n",
    "\n",
    "        # don't comment this block\n",
    "        try:\n",
    "            del pred_distance_dict[\"rdf-schema#label\"]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return pred_distance_dict\n",
    "\n",
    "    def forcely_search(self, question_list, is_when):\n",
    "        for word in self.all_delete_words:\n",
    "            try:\n",
    "                question_list.remove(word)\n",
    "            except:\n",
    "                pass\n",
    "        possible_word = copy.deepcopy(question_list)\n",
    "        for i in range(len(possible_word)):\n",
    "            possible_word[i] = (\n",
    "                possible_word[i].replace(\"?\", \"\").replace('\"', \"\").replace(\"'\", \"\")\n",
    "            )\n",
    "\n",
    "        possible_relation_word_first = possible_word[0]\n",
    "        possible_entity_word_first = \" \".join(possible_word[1:])\n",
    "        possible_relation_word_last = possible_word[-1]\n",
    "        possible_entity_word_last = \" \".join(possible_word[0:-1])\n",
    "\n",
    "        if is_when == 1:\n",
    "            possible_relation_word_first += \" time\"\n",
    "            possible_relation_word_last += +\" time\"\n",
    "\n",
    "        possible_answer_list1 = self.search_answer_for_all_O(\n",
    "            possible_entity_word_first, possible_relation_word_first\n",
    "        )\n",
    "        possible_answer_list2 = self.search_answer_for_all_O(\n",
    "            possible_entity_word_last, possible_relation_word_last\n",
    "        )\n",
    "\n",
    "        possible_answer = []\n",
    "        if len(possible_answer_list1) != 0:\n",
    "            possible_answer = possible_answer_list1\n",
    "        if len(possible_answer_list2) != 0:\n",
    "            possible_answer = possible_answer_list2\n",
    "\n",
    "        return possible_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading graph ...\n",
      "loaded graph successfully!\n"
     ]
    }
   ],
   "source": [
    "graph_operator = GraphOperations()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate other answer\n",
      "[['O', 'O', 'O', 'O', 'O', 'B-org', 'B-per', 'I-per', 'I-per', 'I-per', 'I-per', 'O', 'O', 'B-org']]\n",
      "[0]\n",
      "Who\n",
      "['director']\n",
      "entity matching for Who\n",
      "relation matching for director\n",
      "\n",
      "                        PREFIX ddis: <http://ddis.ch/atai/>\n",
      "                        PREFIX wd: <http://www.wikidata.org/entity/>\n",
      "                        PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
      "                        PREFIX schema: <http://schema.org/>\n",
      "                        SELECT ?entity_name WHERE{    \n",
      "                            wd:Q58880906 wdt:P57 ?temp.\n",
      "                            ?temp rdfs:label ?entity_name.\n",
      "                        }\n",
      "                        LIMIT 1\n",
      "                    \n",
      "message in sparql\n",
      "\n",
      "                        PREFIX ddis: <http://ddis.ch/atai/>\n",
      "                        PREFIX wd: <http://www.wikidata.org/entity/>\n",
      "                        PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
      "                        PREFIX schema: <http://schema.org/>\n",
      "                        SELECT ?entity_name WHERE{    \n",
      "                            wd:Q58880906 wdt:P57 ?temp.\n",
      "                            ?temp rdfs:label ?entity_name.\n",
      "                        }\n",
      "                        LIMIT 1\n",
      "                    \n",
      "[]\n",
      "[]\n",
      "\n",
      "                        PREFIX ddis: <http://ddis.ch/atai/>\n",
      "                        PREFIX wd: <http://www.wikidata.org/entity/>\n",
      "                        PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
      "                        PREFIX schema: <http://schema.org/>\n",
      "                        SELECT ?entity_name WHERE{    \n",
      "                            wd:Q58880906 wdt:P170 ?temp.\n",
      "                            ?temp rdfs:label ?entity_name.\n",
      "                        }\n",
      "                        LIMIT 1\n",
      "                    \n",
      "message in sparql\n",
      "\n",
      "                        PREFIX ddis: <http://ddis.ch/atai/>\n",
      "                        PREFIX wd: <http://www.wikidata.org/entity/>\n",
      "                        PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
      "                        PREFIX schema: <http://schema.org/>\n",
      "                        SELECT ?entity_name WHERE{    \n",
      "                            wd:Q58880906 wdt:P170 ?temp.\n",
      "                            ?temp rdfs:label ?entity_name.\n",
      "                        }\n",
      "                        LIMIT 1\n",
      "                    \n",
      "['Dr. Seuss']\n",
      "['Dr. Seuss']\n",
      "\n",
      "                        PREFIX ddis: <http://ddis.ch/atai/>\n",
      "                        PREFIX wd: <http://www.wikidata.org/entity/>\n",
      "                        PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
      "                        PREFIX schema: <http://schema.org/>\n",
      "                        SELECT ?entity_name WHERE{    \n",
      "                            wd:Q1324276 wdt:P57 ?temp.\n",
      "                            ?temp rdfs:label ?entity_name.\n",
      "                        }\n",
      "                        LIMIT 1\n",
      "                    \n",
      "message in sparql\n",
      "\n",
      "                        PREFIX ddis: <http://ddis.ch/atai/>\n",
      "                        PREFIX wd: <http://www.wikidata.org/entity/>\n",
      "                        PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
      "                        PREFIX schema: <http://schema.org/>\n",
      "                        SELECT ?entity_name WHERE{    \n",
      "                            wd:Q1324276 wdt:P57 ?temp.\n",
      "                            ?temp rdfs:label ?entity_name.\n",
      "                        }\n",
      "                        LIMIT 1\n",
      "                    \n",
      "['Daisy von Scherler Mayer']\n",
      "['Daisy von Scherler Mayer']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Dr. Seuss'], ['Daisy von Scherler Mayer']]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculator = AnswerCalculator()\n",
    "answer = calculator.calculate_other_answer(\"Who is the director of Star Wars: Episode VI - Return of the Jedi?\", [['O', 'O', 'O', 'O', 'O', 'B-org', 'B-per', 'I-per', 'I-per', 'I-per', 'I-per', 'O', 'O', 'B-org']])\n",
    "\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answers_in_template(s_a_e, s_a_r, s_a, n_k_l, p_k_l):\n",
    "    # first_templates = [\n",
    "    #     \"In my opinion, \",\n",
    "    #     \"As far as I'm concerned, \",\n",
    "    #     \"In my point of view, \",\n",
    "    #     \"Personally speaking, \",\n",
    "    # ]\n",
    "    \n",
    "    first_templates = \"Here is some information I found: \"\n",
    "    \n",
    "    #     R:@@@\n",
    "    #     E:>>>\n",
    "    #     A:<<<\n",
    "    middle_templates = [\"the @@@ of >>> is <<<\", \">>>'s @@@ is <<<\"]\n",
    "    # concatenate_words = [\"Also, \", \"What's more, \", \"In addition, \"]\n",
    "    concatenate_words = \", \"\n",
    "\n",
    "    # number1 = random.randint(0, 3)\n",
    "    # number2 = random.randint(0, 1)\n",
    "    \n",
    "    number2 =random.randint(0, 1)\n",
    "    answer_sentence = first_templates + middle_templates[number2].replace(\n",
    "        \"<<<\", s_a[0][0]\n",
    "    ).replace(\n",
    "        \">>>\", s_a_e[0][0]\n",
    "    ).replace(\n",
    "        \"@@@\", s_a_r[0][0]\n",
    "    )\n",
    "\n",
    "    # if len(s_a_e[0]) == 1: \n",
    "    #     answer_sentence += \". \"\n",
    "    \n",
    "    for i in range(len(s_a_e) - 2):\n",
    "        number2 = 0\n",
    "        number3 = 0\n",
    "        t2 = (\n",
    "            middle_templates[number2]\n",
    "            .replace(\"<<<\", s_a[i + 1][0])\n",
    "            .replace(\">>>\", s_a_e[i + 1][0])\n",
    "            .replace(\"@@@\", s_a_r[i + 1][0])\n",
    "        )\n",
    "        answer_sentence = answer_sentence + concatenate_words + t2\n",
    "        answer_sentence += \" \"\n",
    "\n",
    "    if len(s_a_e) -1 > 0:\n",
    "        answer_sentence += \" and \" + (\n",
    "                middle_templates[number2]\n",
    "                .replace(\"<<<\", s_a[len(s_a_e)-1][0])\n",
    "                .replace(\">>>\", s_a_e[len(s_a_e)-1][0])\n",
    "                .replace(\"@@@\", s_a_r[len(s_a_e)-1][0])\n",
    "            )\n",
    "    \n",
    "    swear_words = \". If you don't believe me, you can check it yourself. I will show you labels of entities and relations. \"\n",
    "    entities_words = str(n_k_l) + \" \"\n",
    "    relations_words = str(p_k_l)\n",
    "    answer_sentence = answer_sentence + swear_words + entities_words + relations_words\n",
    "    return answer_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here is some information I found: the 1 of 1 is 1. If you don't believe me, you can check it yourself. I will show you labels of entities and relations. 4 5\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_a_e, s_a_r, s_a, n_k_l, p_k_l = [[\"1\"]],[[\"1\", \"2\", \"3\", \"4\", \"5\"]],[[\"1\", \"2\", \"3\", \"4\", \"5\"]], \"4\", \"5\"\n",
    "answers_in_template(s_a_e, s_a_r, s_a, n_k_l, p_k_l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
